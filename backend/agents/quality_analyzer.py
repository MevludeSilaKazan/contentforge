"""
ContentForge Quality Analyzer
Ä°Ã§erik kalitesi analiz sistemi

Skorlar:
1. Readability Score - Okunabilirlik
2. SEO Score - Arama motoru optimizasyonu
3. Fact Check Score - Ä°ddia doÄŸrulama
4. Originality Score - Ã–zgÃ¼nlÃ¼k
"""

import re
import math
from typing import Dict, List, Tuple
from groq import Groq
import requests
from config.settings import DEFAULT_MODEL, SERPER_API_KEY


# ============================================================
# READABILITY SCORE - OKUNABÄ°LÄ°RLÄ°K ANALÄ°ZÄ°
# ============================================================

def calculate_readability(content: str) -> Dict:
    """
    TÃ¼rkÃ§e iÃ§erik iÃ§in okunabilirlik analizi
    
    Metrikler:
    - Ortalama cÃ¼mle uzunluÄŸu (ideal: 15-20 kelime)
    - Ortalama kelime uzunluÄŸu (ideal: 5-7 harf)
    - Paragraf uzunluÄŸu (ideal: 3-5 cÃ¼mle)
    - KarmaÅŸÄ±k kelime oranÄ± (3+ hece)
    """
    
    # Markdown ve Ã¶zel karakterleri temizle
    clean_text = re.sub(r'[#*>`\[\]()|\-]', '', content)
    clean_text = re.sub(r'!\[.*?\]\(.*?\)', '', clean_text)  # GÃ¶rselleri kaldÄ±r
    clean_text = re.sub(r'https?://\S+', '', clean_text)  # Linkleri kaldÄ±r
    clean_text = re.sub(r'\s+', ' ', clean_text).strip()
    
    # CÃ¼mleleri ayÄ±r
    sentences = re.split(r'[.!?]+', clean_text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
    
    if not sentences:
        return {"score": 0, "grade": "N/A", "details": {}}
    
    # Kelimeleri say
    words = clean_text.split()
    total_words = len(words)
    total_sentences = len(sentences)
    
    # ParagraflarÄ± say (Ã§ift newline ile ayrÄ±lmÄ±ÅŸ)
    paragraphs = [p.strip() for p in content.split('\n\n') if p.strip() and len(p.strip()) > 20]
    total_paragraphs = max(len(paragraphs), 1)
    
    # Metrikler
    avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0
    avg_word_length = sum(len(w) for w in words) / total_words if total_words > 0 else 0
    avg_paragraph_sentences = total_sentences / total_paragraphs
    
    # KarmaÅŸÄ±k kelimeler (3+ hece - TÃ¼rkÃ§e iÃ§in basit tahmin: 7+ harf)
    complex_words = [w for w in words if len(w) >= 7]
    complex_word_ratio = len(complex_words) / total_words if total_words > 0 else 0
    
    # Skor hesaplama (0-100)
    score = 100
    issues = []
    suggestions = []
    
    # CÃ¼mle uzunluÄŸu deÄŸerlendirmesi
    if avg_sentence_length > 25:
        penalty = min((avg_sentence_length - 25) * 2, 20)
        score -= penalty
        issues.append(f"CÃ¼mleler Ã§ok uzun (ort. {avg_sentence_length:.1f} kelime)")
        suggestions.append("CÃ¼mleleri 15-20 kelimeye kÄ±saltÄ±n")
    elif avg_sentence_length < 10:
        penalty = min((10 - avg_sentence_length) * 2, 15)
        score -= penalty
        issues.append(f"CÃ¼mleler Ã§ok kÄ±sa (ort. {avg_sentence_length:.1f} kelime)")
        suggestions.append("CÃ¼mleleri biraz geniÅŸletin")
    
    # Kelime uzunluÄŸu deÄŸerlendirmesi
    if avg_word_length > 8:
        penalty = min((avg_word_length - 8) * 3, 15)
        score -= penalty
        issues.append("Ã‡ok fazla uzun/teknik kelime")
        suggestions.append("Daha basit kelimeler kullanÄ±n")
    
    # KarmaÅŸÄ±k kelime oranÄ±
    if complex_word_ratio > 0.3:
        penalty = min((complex_word_ratio - 0.3) * 50, 20)
        score -= penalty
        issues.append(f"KarmaÅŸÄ±k kelime oranÄ± yÃ¼ksek (%{complex_word_ratio*100:.0f})")
        suggestions.append("Daha anlaÅŸÄ±lÄ±r kelimeler tercih edin")
    
    # Paragraf uzunluÄŸu
    if avg_paragraph_sentences > 6:
        score -= 10
        issues.append("Paragraflar Ã§ok uzun")
        suggestions.append("ParagraflarÄ± 3-5 cÃ¼mleye bÃ¶lÃ¼n")
    
    # Skor sÄ±nÄ±rlandÄ±rma
    score = max(0, min(100, score))
    
    # Grade belirleme
    if score >= 80:
        grade = "A"
        grade_text = "Ã‡ok Kolay"
    elif score >= 65:
        grade = "B"
        grade_text = "Kolay"
    elif score >= 50:
        grade = "C"
        grade_text = "Orta"
    elif score >= 35:
        grade = "D"
        grade_text = "Zor"
    else:
        grade = "F"
        grade_text = "Ã‡ok Zor"
    
    return {
        "score": round(score),
        "grade": grade,
        "grade_text": grade_text,
        "details": {
            "total_words": total_words,
            "total_sentences": total_sentences,
            "avg_sentence_length": round(avg_sentence_length, 1),
            "avg_word_length": round(avg_word_length, 1),
            "complex_word_ratio": round(complex_word_ratio * 100, 1),
        },
        "issues": issues,
        "suggestions": suggestions
    }


# ============================================================
# SEO SCORE - ARAMA MOTORU OPTÄ°MÄ°ZASYONU
# ============================================================

def calculate_seo_score(content: str, topic: str) -> Dict:
    """
    SEO kalite analizi
    
    Kontroller:
    - BaÅŸlÄ±k uzunluÄŸu ve anahtar kelime
    - Meta aÃ§Ä±klama
    - BaÅŸlÄ±k hiyerarÅŸisi (H1, H2, H3)
    - Anahtar kelime yoÄŸunluÄŸu
    - GÃ¶rsel alt text
    - Ä°Ã§/dÄ±ÅŸ linkler
    - Ä°Ã§erik uzunluÄŸu
    """
    
    score = 100
    checks = []
    issues = []
    suggestions = []
    
    # Anahtar kelimeleri Ã§Ä±kar (topic'ten)
    keywords = [w.lower() for w in topic.split() if len(w) > 2]
    content_lower = content.lower()
    
    # 1. BaÅŸlÄ±k kontrolÃ¼ (H1)
    h1_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
    if h1_match:
        h1_title = h1_match.group(1)
        h1_length = len(h1_title)
        
        if 30 <= h1_length <= 60:
            checks.append(("âœ…", "BaÅŸlÄ±k uzunluÄŸu ideal", f"{h1_length} karakter"))
        elif h1_length < 30:
            score -= 10
            checks.append(("âš ï¸", "BaÅŸlÄ±k Ã§ok kÄ±sa", f"{h1_length} karakter"))
            suggestions.append("BaÅŸlÄ±ÄŸÄ± 30-60 karakter arasÄ±na getirin")
        else:
            score -= 10
            checks.append(("âš ï¸", "BaÅŸlÄ±k Ã§ok uzun", f"{h1_length} karakter"))
            suggestions.append("BaÅŸlÄ±ÄŸÄ± 60 karakterin altÄ±na indirin")
        
        # Anahtar kelime baÅŸlÄ±kta mÄ±?
        keyword_in_title = any(kw in h1_title.lower() for kw in keywords)
        if keyword_in_title:
            checks.append(("âœ…", "Anahtar kelime baÅŸlÄ±kta var", ""))
        else:
            score -= 15
            checks.append(("âŒ", "Anahtar kelime baÅŸlÄ±kta yok", ""))
            suggestions.append(f"'{topic}' ifadesini baÅŸlÄ±ÄŸa ekleyin")
    else:
        score -= 20
        checks.append(("âŒ", "H1 baÅŸlÄ±k bulunamadÄ±", ""))
        issues.append("Ana baÅŸlÄ±k (H1) eksik")
    
    # 2. Meta aÃ§Ä±klama (YAML frontmatter'dan)
    meta_match = re.search(r'aciklama:\s*(.+)', content)
    if meta_match:
        meta_desc = meta_match.group(1)
        meta_length = len(meta_desc)
        
        if 120 <= meta_length <= 160:
            checks.append(("âœ…", "Meta aÃ§Ä±klama ideal", f"{meta_length} karakter"))
        elif meta_length < 120:
            score -= 10
            checks.append(("âš ï¸", "Meta aÃ§Ä±klama kÄ±sa", f"{meta_length} karakter"))
        else:
            score -= 5
            checks.append(("âš ï¸", "Meta aÃ§Ä±klama uzun", f"{meta_length} karakter"))
    else:
        score -= 15
        checks.append(("âŒ", "Meta aÃ§Ä±klama yok", ""))
        suggestions.append("150 karakterlik meta aÃ§Ä±klama ekleyin")
    
    # 3. BaÅŸlÄ±k hiyerarÅŸisi
    h2_count = len(re.findall(r'^##\s+', content, re.MULTILINE))
    h3_count = len(re.findall(r'^###\s+', content, re.MULTILINE))
    
    if h2_count >= 3:
        checks.append(("âœ…", f"{h2_count} alt baÅŸlÄ±k (H2)", ""))
    else:
        score -= 10
        checks.append(("âš ï¸", f"Sadece {h2_count} alt baÅŸlÄ±k", ""))
        suggestions.append("En az 3-4 alt baÅŸlÄ±k ekleyin")
    
    if h3_count >= 2:
        checks.append(("âœ…", f"{h3_count} alt-alt baÅŸlÄ±k (H3)", ""))
    
    # 4. Anahtar kelime yoÄŸunluÄŸu
    word_count = len(content.split())
    keyword_count = sum(content_lower.count(kw) for kw in keywords)
    keyword_density = (keyword_count / word_count * 100) if word_count > 0 else 0
    
    if 1 <= keyword_density <= 3:
        checks.append(("âœ…", f"Anahtar kelime yoÄŸunluÄŸu ideal", f"%{keyword_density:.1f}"))
    elif keyword_density < 1:
        score -= 10
        checks.append(("âš ï¸", "Anahtar kelime az kullanÄ±lmÄ±ÅŸ", f"%{keyword_density:.1f}"))
        suggestions.append(f"'{topic}' ifadesini daha sÄ±k kullanÄ±n")
    else:
        score -= 10
        checks.append(("âš ï¸", "Anahtar kelime fazla kullanÄ±lmÄ±ÅŸ", f"%{keyword_density:.1f}"))
    
    # 5. GÃ¶rsel kontrolÃ¼
    images = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)
    if images:
        checks.append(("âœ…", f"{len(images)} gÃ¶rsel mevcut", ""))
        
        # Alt text kontrolÃ¼
        images_with_alt = [img for img in images if img[0].strip()]
        if len(images_with_alt) == len(images):
            checks.append(("âœ…", "TÃ¼m gÃ¶rsellerde alt text var", ""))
        else:
            score -= 5
            checks.append(("âš ï¸", "BazÄ± gÃ¶rsellerde alt text yok", ""))
    else:
        score -= 10
        checks.append(("âš ï¸", "GÃ¶rsel yok", ""))
        suggestions.append("En az 1-2 gÃ¶rsel ekleyin")
    
    # 6. Link kontrolÃ¼
    links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
    external_links = [l for l in links if l[1].startswith('http')]
    
    if external_links:
        checks.append(("âœ…", f"{len(external_links)} dÄ±ÅŸ link", ""))
    else:
        score -= 5
        checks.append(("âš ï¸", "DÄ±ÅŸ link yok", ""))
    
    # 7. Ä°Ã§erik uzunluÄŸu
    if word_count >= 1500:
        checks.append(("âœ…", f"Ä°Ã§erik uzunluÄŸu ideal", f"{word_count} kelime"))
    elif word_count >= 800:
        checks.append(("âš ï¸", f"Ä°Ã§erik biraz kÄ±sa", f"{word_count} kelime"))
        score -= 5
    else:
        checks.append(("âŒ", f"Ä°Ã§erik Ã§ok kÄ±sa", f"{word_count} kelime"))
        score -= 15
        suggestions.append("En az 1000 kelimelik iÃ§erik hedefleyin")
    
    # Skor sÄ±nÄ±rlandÄ±rma
    score = max(0, min(100, score))
    
    # Grade belirleme
    if score >= 80:
        grade = "A"
    elif score >= 65:
        grade = "B"
    elif score >= 50:
        grade = "C"
    elif score >= 35:
        grade = "D"
    else:
        grade = "F"
    
    return {
        "score": round(score),
        "grade": grade,
        "checks": checks,
        "issues": issues,
        "suggestions": suggestions,
        "details": {
            "word_count": word_count,
            "h2_count": h2_count,
            "h3_count": h3_count,
            "image_count": len(images),
            "link_count": len(external_links),
            "keyword_density": round(keyword_density, 1)
        }
    }


# ============================================================
# FACT CHECK SCORE - Ä°DDÄ°A DOÄRULAMA
# ============================================================

def extract_claims(client: Groq, content: str) -> List[str]:
    """Ä°Ã§erikten doÄŸrulanabilir iddialarÄ± Ã§Ä±kar"""
    
    system = """Ä°Ã§erikten DOÄRULANABILIR iddialarÄ± Ã§Ä±kar. Sadece:
- Ä°statistikler ve rakamlar
- Tarihsel olaylar
- Åirket/kiÅŸi hakkÄ±nda somut bilgiler
- AraÅŸtÄ±rma sonuÃ§larÄ±

Her satÄ±ra bir iddia yaz. Maksimum 5 iddia.
Genel gÃ¶rÃ¼ÅŸleri veya Ã¶znel ifadeleri ALMA."""

    user = f"Ä°Ã§erik:\n{content[:3000]}"
    
    try:
        response = client.chat.completions.create(
            model=DEFAULT_MODEL,
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            temperature=0.2,
            max_tokens=500,
        )
        
        claims = response.choices[0].message.content.strip().split('\n')
        claims = [c.strip('- ').strip() for c in claims if c.strip() and len(c.strip()) > 10]
        return claims[:5]
    except:
        return []


def verify_claim(claim: str) -> Dict:
    """Bir iddiayÄ± web aramasÄ±yla doÄŸrula"""
    
    if not SERPER_API_KEY:
        return {"verified": None, "confidence": 0, "source": None}
    
    try:
        response = requests.post(
            "https://google.serper.dev/search",
            headers={"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"},
            json={"q": claim, "gl": "tr", "hl": "tr", "num": 3},
            timeout=10
        )
        response.raise_for_status()
        data = response.json()
        
        results = data.get("organic", [])
        if not results:
            return {"verified": None, "confidence": 0, "source": None}
        
        # Basit eÅŸleÅŸme kontrolÃ¼
        claim_words = set(claim.lower().split())
        best_match = 0
        best_source = None
        
        for r in results:
            snippet = r.get("snippet", "").lower()
            snippet_words = set(snippet.split())
            
            # Ortak kelime oranÄ±
            common = len(claim_words & snippet_words)
            match_ratio = common / len(claim_words) if claim_words else 0
            
            if match_ratio > best_match:
                best_match = match_ratio
                best_source = r.get("link", "")
        
        if best_match > 0.5:
            return {"verified": True, "confidence": round(best_match * 100), "source": best_source}
        elif best_match > 0.3:
            return {"verified": None, "confidence": round(best_match * 100), "source": best_source}
        else:
            return {"verified": False, "confidence": round(best_match * 100), "source": None}
    
    except:
        return {"verified": None, "confidence": 0, "source": None}


def calculate_fact_check_score(client: Groq, content: str) -> Dict:
    """
    Fact-check analizi
    
    1. Ä°Ã§erikten iddialarÄ± Ã§Ä±kar
    2. Her iddiayÄ± web'de ara
    3. DoÄŸrulama skoru hesapla
    """
    
    # Ä°ddialarÄ± Ã§Ä±kar
    claims = extract_claims(client, content)
    
    if not claims:
        return {
            "score": 85,  # Ä°ddia yoksa varsayÄ±lan skor
            "grade": "B",
            "claims_checked": 0,
            "verified": 0,
            "unverified": 0,
            "uncertain": 0,
            "details": [],
            "note": "DoÄŸrulanabilir somut iddia bulunamadÄ±"
        }
    
    # Her iddiayÄ± doÄŸrula
    results = []
    verified = 0
    unverified = 0
    uncertain = 0
    
    for claim in claims:
        result = verify_claim(claim)
        results.append({
            "claim": claim[:100],
            "verified": result["verified"],
            "confidence": result["confidence"],
            "source": result["source"]
        })
        
        if result["verified"] is True:
            verified += 1
        elif result["verified"] is False:
            unverified += 1
        else:
            uncertain += 1
    
    # Skor hesaplama
    total = len(claims)
    if total > 0:
        score = ((verified * 100) + (uncertain * 60) + (unverified * 20)) / total
    else:
        score = 85
    
    score = max(0, min(100, round(score)))
    
    # Grade
    if score >= 80:
        grade = "A"
    elif score >= 65:
        grade = "B"
    elif score >= 50:
        grade = "C"
    elif score >= 35:
        grade = "D"
    else:
        grade = "F"
    
    return {
        "score": score,
        "grade": grade,
        "claims_checked": total,
        "verified": verified,
        "unverified": unverified,
        "uncertain": uncertain,
        "details": results
    }


# ============================================================
# ORIGINALITY SCORE - Ã–ZGÃœNLÃœK ANALÄ°ZÄ°
# ============================================================

def calculate_originality_score(content: str) -> Dict:
    """
    Ã–zgÃ¼nlÃ¼k analizi (basit versiyon)
    
    - CÃ¼mle benzersizliÄŸi kontrolÃ¼
    - KliÅŸe/kalÄ±p ifade tespiti
    - Ã–zgÃ¼n ifade oranÄ±
    """
    
    # YaygÄ±n kliÅŸeler
    cliches = [
        "gÃ¼nÃ¼mÃ¼zde", "modern dÃ¼nyada", "hÄ±zla deÄŸiÅŸen",
        "Ã¶nemli bir rol", "bÃ¼yÃ¼k bir Ã¶neme sahip",
        "son yÄ±llarda", "giderek artan", "vazgeÃ§ilmez",
        "kritik Ã¶neme sahip", "hayati Ã¶nem", "dijital Ã§aÄŸda",
        "bir adÄ±m Ã¶nde", "fark yaratmak", "baÅŸarÄ±nÄ±n anahtarÄ±",
        "sonuÃ§ olarak", "Ã¶zetle", "tÃ¼m bunlar gÃ¶steriyor ki"
    ]
    
    # Markdown temizle
    clean_text = re.sub(r'[#*>`\[\]()|\-]', '', content)
    clean_text = re.sub(r'!\[.*?\]\(.*?\)', '', clean_text)
    clean_text = re.sub(r'https?://\S+', '', clean_text)
    clean_text = clean_text.lower()
    
    # KliÅŸe sayÄ±sÄ±
    cliche_count = 0
    found_cliches = []
    for cliche in cliches:
        count = clean_text.count(cliche)
        if count > 0:
            cliche_count += count
            found_cliches.append(cliche)
    
    # CÃ¼mleleri kontrol et
    sentences = re.split(r'[.!?]+', clean_text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 20]
    total_sentences = len(sentences)
    
    # Benzersiz cÃ¼mle oranÄ± (tekrar eden cÃ¼mleler)
    unique_sentences = len(set(sentences))
    uniqueness_ratio = unique_sentences / total_sentences if total_sentences > 0 else 1
    
    # Skor hesaplama
    score = 100
    issues = []
    suggestions = []
    
    # KliÅŸe cezasÄ±
    word_count = len(clean_text.split())
    cliche_ratio = cliche_count / (word_count / 100) if word_count > 0 else 0
    
    if cliche_ratio > 3:
        penalty = min(cliche_ratio * 5, 30)
        score -= penalty
        issues.append(f"{cliche_count} kliÅŸe ifade bulundu")
        suggestions.append(f"Åu ifadeleri deÄŸiÅŸtirin: {', '.join(found_cliches[:3])}")
    elif cliche_ratio > 1.5:
        score -= 10
        issues.append(f"{cliche_count} kliÅŸe ifade var")
    
    # Tekrar cezasÄ±
    if uniqueness_ratio < 0.9:
        penalty = (1 - uniqueness_ratio) * 50
        score -= penalty
        issues.append("Tekrar eden cÃ¼mleler var")
        suggestions.append("Benzer cÃ¼mleleri farklÄ± ÅŸekilde ifade edin")
    
    score = max(0, min(100, round(score)))
    
    # Grade
    if score >= 85:
        grade = "A"
        grade_text = "YÃ¼ksek Ã–zgÃ¼nlÃ¼k"
    elif score >= 70:
        grade = "B"
        grade_text = "Ä°yi Ã–zgÃ¼nlÃ¼k"
    elif score >= 55:
        grade = "C"
        grade_text = "Orta Ã–zgÃ¼nlÃ¼k"
    elif score >= 40:
        grade = "D"
        grade_text = "DÃ¼ÅŸÃ¼k Ã–zgÃ¼nlÃ¼k"
    else:
        grade = "F"
        grade_text = "Ã‡ok DÃ¼ÅŸÃ¼k"
    
    return {
        "score": score,
        "grade": grade,
        "grade_text": grade_text,
        "details": {
            "cliche_count": cliche_count,
            "found_cliches": found_cliches[:5],
            "unique_sentence_ratio": round(uniqueness_ratio * 100, 1)
        },
        "issues": issues,
        "suggestions": suggestions
    }


# ============================================================
# ANA ANALÄ°Z FONKSÄ°YONU
# ============================================================

def analyze_content_quality(content: str, topic: str, deep_check: bool = True) -> Dict:
    """
    TÃ¼m kalite metriklerini hesapla
    
    Args:
        content: Blog iÃ§eriÄŸi
        topic: Blog konusu
        deep_check: Fact-check yapÄ±lsÄ±n mÄ± (API kullanÄ±r)
    
    Returns:
        TÃ¼m skorlarÄ± iÃ§eren dict
    """
    
    # 1. Okunabilirlik
    readability = calculate_readability(content)
    
    # 2. SEO
    seo = calculate_seo_score(content, topic)
    
    # 3. Ã–zgÃ¼nlÃ¼k
    originality = calculate_originality_score(content)
    
    # 4. Fact-check (opsiyonel)
    if deep_check and SERPER_API_KEY:
        try:
            client = Groq()
            fact_check = calculate_fact_check_score(client, content)
        except:
            fact_check = {"score": 0, "grade": "N/A", "note": "Analiz yapÄ±lamadÄ±"}
    else:
        fact_check = {"score": 0, "grade": "N/A", "note": "Devre dÄ±ÅŸÄ±"}
    
    # Genel skor (aÄŸÄ±rlÄ±klÄ± ortalama)
    weights = {
        "readability": 0.25,
        "seo": 0.30,
        "originality": 0.25,
        "fact_check": 0.20
    }
    
    scores = {
        "readability": readability["score"],
        "seo": seo["score"],
        "originality": originality["score"],
        "fact_check": fact_check["score"] if fact_check["score"] > 0 else 75  # Default
    }
    
    overall_score = sum(scores[k] * weights[k] for k in weights)
    overall_score = round(overall_score)
    
    # Genel grade
    if overall_score >= 80:
        overall_grade = "A"
    elif overall_score >= 65:
        overall_grade = "B"
    elif overall_score >= 50:
        overall_grade = "C"
    elif overall_score >= 35:
        overall_grade = "D"
    else:
        overall_grade = "F"
    
    return {
        "overall": {
            "score": overall_score,
            "grade": overall_grade
        },
        "readability": readability,
        "seo": seo,
        "originality": originality,
        "fact_check": fact_check
    }


# ============================================================
# QUALITY REPORT GENERATOR
# ============================================================

def generate_quality_report(quality_data: Dict) -> str:
    """Kalite raporunu markdown formatÄ±nda oluÅŸtur"""
    
    overall = quality_data["overall"]
    
    report = f"""
## ğŸ“Š Ä°Ã§erik Kalite Raporu

### Genel Skor: {overall['score']}/100 ({overall['grade']})

| Metrik | Skor | Grade |
|--------|------|-------|
| ğŸ“– Okunabilirlik | {quality_data['readability']['score']} | {quality_data['readability']['grade']} |
| ğŸ” SEO | {quality_data['seo']['score']} | {quality_data['seo']['grade']} |
| âœ¨ Ã–zgÃ¼nlÃ¼k | {quality_data['originality']['score']} | {quality_data['originality']['grade']} |
| âœ… DoÄŸruluk | {quality_data['fact_check']['score']} | {quality_data['fact_check']['grade']} |

"""
    
    # Ä°yileÅŸtirme Ã¶nerileri
    all_suggestions = []
    all_suggestions.extend(quality_data['readability'].get('suggestions', []))
    all_suggestions.extend(quality_data['seo'].get('suggestions', []))
    all_suggestions.extend(quality_data['originality'].get('suggestions', []))
    
    if all_suggestions:
        report += "### ğŸ’¡ Ä°yileÅŸtirme Ã–nerileri\n"
        for i, suggestion in enumerate(all_suggestions[:5], 1):
            report += f"{i}. {suggestion}\n"
    
    return report
