"""
ContentForge Blog Agents - V10 Deep Research Edition
GeliÅŸmiÅŸ Ã§ok katmanlÄ± araÅŸtÄ±rma sistemi

AraÅŸtÄ±rma KatmanlarÄ±:
1. ðŸŒ Genel Bilgi - Temel kavramlar ve tanÄ±mlar
2. ðŸ“Š Ä°statistik & Veri - Rakamlar, yÃ¼zdeler, trendler
3. ðŸ“° GÃ¼ncel Haberler - Son geliÅŸmeler
4. ðŸŽ“ Uzman GÃ¶rÃ¼ÅŸleri - Akademik ve profesyonel kaynaklar
5. ðŸ’¼ Vaka Ã‡alÄ±ÅŸmalarÄ± - GerÃ§ek Ã¶rnekler ve baÅŸarÄ± hikayeleri
6. ðŸŒ Global Kaynaklar - Ä°ngilizce araÅŸtÄ±rma
7. â“ SSS & Sorunlar - SÄ±k sorulan sorular ve Ã§Ã¶zÃ¼mler
"""

from groq import Groq
from typing import List, Dict, Optional, Generator, Any
import os
import re
import json
import requests
from datetime import datetime
from config.settings import DEFAULT_MODEL, SERPER_API_KEY


# ============================================================
# AGENT TANIMLARI
# ============================================================

AGENTS = {
    "researcher": {
        "id": "researcher",
        "name": "AraÅŸtÄ±rmacÄ±",
        "name_en": "Deep Researcher",
        "avatar": "ðŸ”",
        "color": "#3B82F6",
        "description": "7 katmanlÄ± derinlemesine araÅŸtÄ±rma yapÄ±yor",
        "tasks": ["Ã‡oklu kaynak", "Ä°statistik toplama", "Global araÅŸtÄ±rma"]
    },
    "visual_curator": {
        "id": "visual_curator",
        "name": "GÃ¶rsel UzmanÄ±",
        "name_en": "Visual Curator",
        "avatar": "ðŸ–¼ï¸",
        "color": "#8B5CF6",
        "description": "En uygun gÃ¶rselleri seÃ§iyor",
        "tasks": ["GÃ¶rsel arama", "Uygunluk kontrolÃ¼", "Lisans kontrolÃ¼"]
    },
    "writer": {
        "id": "writer",
        "name": "Yazar",
        "name_en": "Writer",
        "avatar": "âœï¸",
        "color": "#10B981",
        "description": "Ä°Ã§eriÄŸi oluÅŸturuyor",
        "tasks": ["YapÄ± oluÅŸturma", "Ä°Ã§erik yazma", "Ã–rnekler ekleme"]
    },
    "editor": {
        "id": "editor",
        "name": "EditÃ¶r",
        "name_en": "Editor",
        "avatar": "âœ¨",
        "color": "#F59E0B",
        "description": "Ä°Ã§eriÄŸi dÃ¼zenliyor ve iyileÅŸtiriyor",
        "tasks": ["Dil kontrolÃ¼", "SEO optimizasyonu", "Format dÃ¼zenleme"]
    },
    "quality_analyst": {
        "id": "quality_analyst",
        "name": "Kalite Analisti",
        "name_en": "Quality Analyst",
        "avatar": "ðŸ“Š",
        "color": "#EF4444",
        "description": "Kalite skorlarÄ±nÄ± hesaplÄ±yor",
        "tasks": ["Okunabilirlik", "SEO skoru", "Ã–zgÃ¼nlÃ¼k analizi"]
    }
}


# ============================================================
# ARAÅžTIRMA KATEGORÄ°LERÄ°
# ============================================================

RESEARCH_CATEGORIES = {
    "general": {
        "icon": "ðŸŒ",
        "name": "Genel Bilgi",
        "description": "Temel kavramlar ve tanÄ±mlar",
        "priority": 1
    },
    "statistics": {
        "icon": "ðŸ“Š",
        "name": "Ä°statistik & Veri",
        "description": "Rakamlar, yÃ¼zdeler, pazar verileri",
        "priority": 2
    },
    "news": {
        "icon": "ðŸ“°",
        "name": "GÃ¼ncel Haberler",
        "description": "Son geliÅŸmeler ve trendler",
        "priority": 3
    },
    "expert": {
        "icon": "ðŸŽ“",
        "name": "Uzman GÃ¶rÃ¼ÅŸleri",
        "description": "Akademik ve profesyonel kaynaklar",
        "priority": 4
    },
    "cases": {
        "icon": "ðŸ’¼",
        "name": "Vaka Ã‡alÄ±ÅŸmalarÄ±",
        "description": "GerÃ§ek Ã¶rnekler ve baÅŸarÄ± hikayeleri",
        "priority": 5
    },
    "global": {
        "icon": "ðŸŒ",
        "name": "Global Kaynaklar",
        "description": "UluslararasÄ± araÅŸtÄ±rma",
        "priority": 6
    },
    "faq": {
        "icon": "â“",
        "name": "SSS & Sorunlar",
        "description": "SÄ±k sorulan sorular",
        "priority": 7
    }
}


# ============================================================
# UNSPLASH API
# ============================================================

UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY", "")

def search_images(query: str, count: int = 3) -> List[Dict]:
    if not UNSPLASH_ACCESS_KEY:
        return []
    
    try:
        response = requests.get(
            "https://api.unsplash.com/search/photos",
            params={"query": query, "per_page": count, "orientation": "landscape"},
            headers={"Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}"},
            timeout=10
        )
        response.raise_for_status()
        data = response.json()
        
        images = []
        for photo in data.get("results", []):
            images.append({
                "url": photo["urls"]["regular"],
                "thumb": photo["urls"]["thumb"],
                "alt": photo.get("alt_description", query),
                "credit": photo["user"]["name"],
                "credit_link": photo["user"]["links"]["html"]
            })
        return images
    except:
        return []


def get_images_for_topic(topic: str, sections: List[str] = None) -> Dict[str, Dict]:
    images = {}
    
    hero_images = search_images(topic, count=1)
    if hero_images:
        images["hero"] = hero_images[0]
    
    if sections:
        for section in sections[:5]:
            section_images = search_images(f"{section} {topic}", count=1)
            if section_images:
                images[section] = section_images[0]
    
    return images


# ============================================================
# GELÄ°ÅžMÄ°Åž WEB ARAMA SÄ°STEMÄ°
# ============================================================

def web_search(query: str, num_results: int = 10, language: str = "tr", 
               search_type: str = "search", time_range: str = None) -> List[Dict]:
    """
    GeliÅŸmiÅŸ web arama fonksiyonu
    
    Args:
        query: Arama sorgusu
        num_results: SonuÃ§ sayÄ±sÄ±
        language: Dil (tr/en)
        search_type: Arama tipi (search/news)
        time_range: Zaman aralÄ±ÄŸÄ± (d=gÃ¼n, w=hafta, m=ay, y=yÄ±l)
    """
    if not SERPER_API_KEY:
        return []
    
    try:
        # Endpoint belirleme
        endpoint = "https://google.serper.dev/search"
        if search_type == "news":
            endpoint = "https://google.serper.dev/news"
        
        # Request parametreleri
        params = {
            "q": query,
            "gl": "tr" if language == "tr" else "us",
            "hl": language,
            "num": num_results
        }
        
        # Zaman filtresi
        if time_range:
            params["tbs"] = f"qdr:{time_range}"
        
        response = requests.post(
            endpoint,
            headers={"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"},
            json=params,
            timeout=15
        )
        response.raise_for_status()
        data = response.json()
        
        results = []
        
        # Organik sonuÃ§lar
        for item in data.get("organic", []):
            results.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "link": item.get("link", ""),
                "date": item.get("date", ""),
                "source": extract_domain(item.get("link", ""))
            })
        
        # News sonuÃ§larÄ±
        for item in data.get("news", []):
            results.append({
                "title": item.get("title", ""),
                "snippet": item.get("snippet", ""),
                "link": item.get("link", ""),
                "date": item.get("date", ""),
                "source": item.get("source", "")
            })
        
        # Knowledge Graph
        if "knowledgeGraph" in data:
            kg = data["knowledgeGraph"]
            kg_result = {
                "title": kg.get("title", ""),
                "snippet": kg.get("description", ""),
                "link": kg.get("website", ""),
                "source": "Knowledge Graph",
                "is_kg": True,
                "attributes": kg.get("attributes", {})
            }
            results.insert(0, kg_result)
        
        # Answer Box
        if "answerBox" in data:
            ab = data["answerBox"]
            answer_result = {
                "title": ab.get("title", "DoÄŸrudan Cevap"),
                "snippet": ab.get("answer", ab.get("snippet", "")),
                "link": ab.get("link", ""),
                "source": "Answer Box",
                "is_answer": True
            }
            results.insert(0, answer_result)
        
        # People Also Ask
        if "peopleAlsoAsk" in data:
            for paa in data["peopleAlsoAsk"][:3]:
                results.append({
                    "title": paa.get("question", ""),
                    "snippet": paa.get("snippet", ""),
                    "link": paa.get("link", ""),
                    "source": "Ä°lgili Soru",
                    "is_question": True
                })
        
        return results
        
    except Exception as e:
        print(f"Arama hatasÄ±: {e}")
        return []


def extract_domain(url: str) -> str:
    """URL'den domain Ã§Ä±karÄ±r"""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.replace("www.", "")
        return domain
    except:
        return ""


def extract_statistics(text: str) -> List[str]:
    """Metinden istatistikleri Ã§Ä±karÄ±r"""
    stats = []
    
    # YÃ¼zde kalÄ±plarÄ±
    percentages = re.findall(r'%\s*\d+[\.,]?\d*|\d+[\.,]?\d*\s*%', text)
    stats.extend([f"ðŸ“ˆ {p.strip()}" for p in percentages])
    
    # BÃ¼yÃ¼k sayÄ±lar (milyon, milyar, trilyon)
    big_numbers = re.findall(r'\d+[\.,]?\d*\s*(milyon|milyar|trilyon|million|billion|trillion)', text, re.IGNORECASE)
    stats.extend([f"ðŸ’° {n[0]} {n[1]}" if isinstance(n, tuple) else f"ðŸ’° {n}" for n in big_numbers])
    
    # Para birimleri
    money = re.findall(r'[\$â‚¬â‚º]\s*\d+[\.,]?\d*\s*(milyon|milyar|bin|K|M|B)?', text, re.IGNORECASE)
    
    # YÄ±l bazlÄ± veriler
    year_data = re.findall(r'20\d{2}\s*[-â€“]\s*20\d{2}|20\d{2}\s+yÄ±lÄ±nda', text)
    stats.extend([f"ðŸ“… {y}" for y in year_data])
    
    return list(set(stats))[:10]  # Maksimum 10 istatistik


def extract_quotes(text: str) -> List[str]:
    """Metinden alÄ±ntÄ±larÄ± Ã§Ä±karÄ±r"""
    quotes = []
    
    # TÄ±rnak iÃ§indeki metinler (Ã§eÅŸitli tÄ±rnak stilleri)
    # Standart Ã§ift tÄ±rnak
    quoted = re.findall(r'"([^"]{20,200})"', text)
    # Tek tÄ±rnak
    quoted += re.findall(r"'([^']{20,200})'", text)
    # Guillemet tÄ±rnaklar
    quoted += re.findall(r'Â«([^Â»]{20,200})Â»', text)
    
    for q in quoted[:5]:
        quotes.append(f'ðŸ’¬ "{q}"')
    
    return quotes


# ============================================================
# 7 KATMANLI ARAÅžTIRMA SÄ°STEMÄ°
# ============================================================

def deep_research(topic: str, format_type: str = "standard") -> Dict[str, Any]:
    """
    7 katmanlÄ± derinlemesine araÅŸtÄ±rma sistemi
    
    Returns:
        {
            "layers": {...},
            "statistics": [...],
            "quotes": [...],
            "sources_count": int,
            "compiled_research": str
        }
    """
    
    research_data = {
        "layers": {},
        "statistics": [],
        "quotes": [],
        "sources": [],
        "sources_count": 0
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 1: GENEL BÄ°LGÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    general_queries = [
        f"{topic} nedir",
        f"{topic} tanÄ±mÄ± ve Ã¶nemi",
        f"{topic} temel kavramlar"
    ]
    
    layer_results = []
    for query in general_queries:
        results = web_search(query, num_results=5, language="tr")
        layer_results.extend(results)
    
    research_data["layers"]["general"] = {
        "category": RESEARCH_CATEGORIES["general"],
        "results": layer_results[:8],
        "query_count": len(general_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 2: Ä°STATÄ°STÄ°K & VERÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    stats_queries = [
        f"{topic} istatistikleri 2024",
        f"{topic} pazar bÃ¼yÃ¼klÃ¼ÄŸÃ¼",
        f"{topic} araÅŸtÄ±rma verileri",
        f"{topic} yÃ¼zde oran rakamlar"
    ]
    
    layer_results = []
    for query in stats_queries:
        results = web_search(query, num_results=5, language="tr")
        layer_results.extend(results)
        
        # Ä°statistik Ã§Ä±karma
        for r in results:
            stats = extract_statistics(r.get("snippet", ""))
            research_data["statistics"].extend(stats)
    
    research_data["layers"]["statistics"] = {
        "category": RESEARCH_CATEGORIES["statistics"],
        "results": layer_results[:8],
        "query_count": len(stats_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 3: GÃœNCEL HABERLER
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    news_queries = [
        f"{topic} son geliÅŸmeler",
        f"{topic} 2024 haberleri"
    ]
    
    layer_results = []
    for query in news_queries:
        # Son 1 aylÄ±k haberler
        results = web_search(query, num_results=5, language="tr", search_type="news", time_range="m")
        layer_results.extend(results)
    
    research_data["layers"]["news"] = {
        "category": RESEARCH_CATEGORIES["news"],
        "results": layer_results[:6],
        "query_count": len(news_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 4: UZMAN GÃ–RÃœÅžLERÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    expert_queries = [
        f"{topic} uzman gÃ¶rÃ¼ÅŸÃ¼",
        f"{topic} profesyonel tavsiye",
        f'"{topic}" CEO aÃ§Ä±klama'
    ]
    
    layer_results = []
    for query in expert_queries:
        results = web_search(query, num_results=5, language="tr")
        layer_results.extend(results)
        
        # AlÄ±ntÄ± Ã§Ä±karma
        for r in results:
            quotes = extract_quotes(r.get("snippet", ""))
            research_data["quotes"].extend(quotes)
    
    research_data["layers"]["expert"] = {
        "category": RESEARCH_CATEGORIES["expert"],
        "results": layer_results[:6],
        "query_count": len(expert_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 5: VAKA Ã‡ALIÅžMALARI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    case_queries = [
        f"{topic} baÅŸarÄ± hikayesi",
        f"{topic} Ã¶rnek ÅŸirket",
        f"{topic} vaka Ã§alÄ±ÅŸmasÄ± case study"
    ]
    
    layer_results = []
    for query in case_queries:
        results = web_search(query, num_results=5, language="tr")
        layer_results.extend(results)
    
    research_data["layers"]["cases"] = {
        "category": RESEARCH_CATEGORIES["cases"],
        "results": layer_results[:6],
        "query_count": len(case_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 6: GLOBAL KAYNAKLAR (Ä°NGÄ°LÄ°ZCE)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # TÃ¼rkÃ§e konuyu Ä°ngilizceye Ã§evir (basit yaklaÅŸÄ±m)
    topic_en = topic  # Ä°leride Ã§eviri API eklenebilir
    
    global_queries = [
        f"{topic_en} statistics 2024",
        f"{topic_en} trends research",
        f"{topic_en} best practices"
    ]
    
    layer_results = []
    for query in global_queries:
        results = web_search(query, num_results=5, language="en")
        layer_results.extend(results)
        
        # Ä°ngilizce istatistikler
        for r in results:
            stats = extract_statistics(r.get("snippet", ""))
            research_data["statistics"].extend(stats)
    
    research_data["layers"]["global"] = {
        "category": RESEARCH_CATEGORIES["global"],
        "results": layer_results[:6],
        "query_count": len(global_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KATMAN 7: SSS & SORUNLAR
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    faq_queries = [
        f"{topic} sÄ±k sorulan sorular",
        f"{topic} sorunlarÄ± Ã§Ã¶zÃ¼mleri",
        f"{topic} nasÄ±l yapÄ±lÄ±r"
    ]
    
    layer_results = []
    for query in faq_queries:
        results = web_search(query, num_results=5, language="tr")
        layer_results.extend(results)
    
    research_data["layers"]["faq"] = {
        "category": RESEARCH_CATEGORIES["faq"],
        "results": layer_results[:6],
        "query_count": len(faq_queries)
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FORMAT BAZLI EK ARAÅžTIRMA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    format_extra_queries = {
        "listicle": [f"{topic} en iyi yollarÄ±", f"{topic} ipuÃ§larÄ± listesi"],
        "howto": [f"{topic} adÄ±m adÄ±m rehber", f"{topic} baÅŸlangÄ±Ã§ kÄ±lavuzu"],
        "comparison": [f"{topic} karÅŸÄ±laÅŸtÄ±rma", f"{topic} alternatifleri vs"],
        "casestudy": [f"{topic} ROI sonuÃ§lar", f"{topic} dÃ¶nÃ¼ÅŸÃ¼m metrikleri"]
    }
    
    if format_type in format_extra_queries:
        extra_results = []
        for query in format_extra_queries[format_type]:
            results = web_search(query, num_results=5, language="tr")
            extra_results.extend(results)
        
        research_data["layers"]["format_specific"] = {
            "category": {"icon": "ðŸŽ¯", "name": f"{format_type.title()} Ã–zel", "description": "Format bazlÄ± araÅŸtÄ±rma"},
            "results": extra_results[:6],
            "query_count": len(format_extra_queries[format_type])
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SONUÃ‡LARI DERLÄ°ME
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Toplam kaynak sayÄ±sÄ±
    total_sources = 0
    for layer_name, layer_data in research_data["layers"].items():
        total_sources += len(layer_data["results"])
        for result in layer_data["results"]:
            if result.get("source") and result["source"] not in research_data["sources"]:
                research_data["sources"].append(result["source"])
    
    research_data["sources_count"] = total_sources
    
    # Ä°statistikleri benzersizleÅŸtir
    research_data["statistics"] = list(set(research_data["statistics"]))[:15]
    
    # AlÄ±ntÄ±larÄ± benzersizleÅŸtir
    research_data["quotes"] = list(set(research_data["quotes"]))[:8]
    
    # DerlenmiÅŸ araÅŸtÄ±rma metni oluÅŸtur
    research_data["compiled_research"] = compile_research_text(research_data)
    
    return research_data


def compile_research_text(research_data: Dict) -> str:
    """AraÅŸtÄ±rma verilerini derlenmiÅŸ metin haline getirir"""
    
    sections = []
    
    # BaÅŸlÄ±k
    sections.append("# ðŸ“š DERÄ°NLEMESÄ°NE ARAÅžTIRMA RAPORU\n")
    sections.append(f"**Toplam Kaynak:** {research_data['sources_count']} | **Benzersiz Site:** {len(research_data['sources'])}\n")
    
    # Ä°statistik Ã¶zeti
    if research_data["statistics"]:
        sections.append("\n## ðŸ“Š BULUNAN Ä°STATÄ°STÄ°KLER")
        for stat in research_data["statistics"][:10]:
            sections.append(f"- {stat}")
    
    # AlÄ±ntÄ±lar
    if research_data["quotes"]:
        sections.append("\n## ðŸ’¬ UZMAN ALINTILARI")
        for quote in research_data["quotes"][:5]:
            sections.append(f"- {quote}")
    
    # Her katmandan bilgiler
    for layer_name, layer_data in research_data["layers"].items():
        category = layer_data["category"]
        results = layer_data["results"]
        
        if results:
            sections.append(f"\n## {category['icon']} {category['name'].upper()}")
            sections.append(f"*{category['description']}*\n")
            
            for r in results[:5]:
                title = r.get("title", "")
                snippet = r.get("snippet", "")
                link = r.get("link", "")
                source = r.get("source", "")
                date = r.get("date", "")
                
                if title and snippet:
                    sections.append(f"**{title}**")
                    sections.append(f"{snippet}")
                    if date:
                        sections.append(f"ðŸ“… {date}")
                    if source:
                        sections.append(f"ðŸ”— Kaynak: {source}")
                    if link:
                        sections.append(f"ðŸ“Ž {link}")
                    sections.append("")
    
    return "\n".join(sections)


# ============================================================
# ESKÄ° FONKSÄ°YON - GERÄ°YE UYUMLULUK
# ============================================================

def research_for_format(topic: str, format_type: str) -> str:
    """
    Geriye uyumlu araÅŸtÄ±rma fonksiyonu
    Yeni deep_research sistemini kullanÄ±r
    """
    research_data = deep_research(topic, format_type)
    return research_data["compiled_research"]


# ============================================================
# AYARLAR
# ============================================================

AUDIENCE_CONFIG = {
    "general": {"desc": "Genel okuyucu", "style": "Basit dil, gÃ¼nlÃ¼k Ã¶rnekler"},
    "professional": {"desc": "Profesyonel", "style": "Teknik terimler, derinlemesine analiz"},
    "entrepreneur": {"desc": "GiriÅŸimci", "style": "ROI odaklÄ±, iÅŸ deÄŸeri vurgula"},
    "technical": {"desc": "Teknik uzman", "style": "DetaylÄ± metodoloji, teknik derinlik"}
}

TONE_CONFIG = {
    "formal": "Resmi, akademik, profesyonel",
    "friendly": "Samimi, sÄ±cak, sohbet havasÄ±",
    "educational": "EÄŸitici, adÄ±m adÄ±m, Ã¶ÄŸretici",
    "persuasive": "Ä°kna edici, faydalarÄ± vurgula"
}

LENGTH_CONFIG = {
    "short": {"words": "800-1000", "sections": 4},
    "medium": {"words": "1500-1800", "sections": 6},
    "long": {"words": "2500-3000", "sections": 8}
}

FORMAT_CONFIG = {
    "standard": {"name": "Standart Blog", "description": "Klasik blog yazÄ±sÄ±", "icon": "ðŸ“"},
    "listicle": {"name": "Listicle", "description": "\"10 Yol\" formatÄ±", "icon": "ðŸ“‹"},
    "howto": {"name": "NasÄ±l YapÄ±lÄ±r", "description": "AdÄ±m adÄ±m rehber", "icon": "ðŸ”§"},
    "comparison": {"name": "KarÅŸÄ±laÅŸtÄ±rma", "description": "X vs Y analizi", "icon": "âš–ï¸"},
    "casestudy": {"name": "Vaka Ã‡alÄ±ÅŸmasÄ±", "description": "DetaylÄ± analiz", "icon": "ðŸ”¬"}
}


# ============================================================
# LLM Ã‡AÄžRISI
# ============================================================

def call_llm(client: Groq, system: str, user: str, temp: float = 0.7) -> str:
    response = client.chat.completions.create(
        model=DEFAULT_MODEL,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": user}
        ],
        temperature=temp,
        max_tokens=6000,
    )
    return response.choices[0].message.content


# ============================================================
# FORMAT YAZIM FONKSÄ°YONLARI
# ============================================================

def _format_images(images: Dict) -> str:
    if not images:
        return "GÃ¶rsel bulunamadÄ±."
    
    lines = []
    for section, img in images.items():
        lines.append(f"- {section}: ![{img['alt']}]({img['url']})")
        lines.append(f"  FotoÄŸraf: {img['credit']}")
    return "\n".join(lines)


def _format_statistics(statistics: List[str]) -> str:
    """Ä°statistikleri prompt iÃ§in formatlar"""
    if not statistics:
        return "Ä°statistik bulunamadÄ±."
    return "\n".join([f"- {s}" for s in statistics[:10]])


def _format_quotes(quotes: List[str]) -> str:
    """AlÄ±ntÄ±larÄ± prompt iÃ§in formatlar"""
    if not quotes:
        return "AlÄ±ntÄ± bulunamadÄ±."
    return "\n".join([f"- {q}" for q in quotes[:5]])


def write_standard(client: Groq, topic: str, research: str, images: Dict,
                   audience: str, tone: str, length: str,
                   statistics: List[str] = None, quotes: List[str] = None) -> str:
    lng = LENGTH_CONFIG[length]
    aud = AUDIENCE_CONFIG[audience]
    image_md = _format_images(images)
    stats_md = _format_statistics(statistics or [])
    quotes_md = _format_quotes(quotes or [])
    
    system = f"""Sen profesyonel bir blog yazarÄ±sÄ±n. AraÅŸtÄ±rma verilerini kullanarak kapsamlÄ± blog yazÄ±sÄ± yaz.

HEDEF: {lng['words']} kelime, {lng['sections']} bÃ¶lÃ¼m
TON: {TONE_CONFIG[tone]}
KÄ°TLE: {aud['desc']} - {aud['style']}

KRÄ°TÄ°K KURALLAR:
1. AÅŸaÄŸÄ±daki istatistikleri MUTLAKA kullan ve kaynak gÃ¶ster
2. Uzman alÄ±ntÄ±larÄ±nÄ± iÃ§eriÄŸe entegre et
3. Her bÃ¶lÃ¼mde somut veri olsun
4. GÃ¼ncel Ã¶rnekler ve trendleri dahil et
5. SEO iÃ§in anahtar kelimeleri doÄŸal kullan

KULLANILACAK Ä°STATÄ°STÄ°KLER:
{stats_md}

KULLANILACAK ALINTILAR:
{quotes_md}

GÃ–RSELLER:
{image_md}

Highlight kutularÄ±, bilgi kutularÄ± ve Ã§aÄŸrÄ± kutularÄ±nÄ± kullan."""

    user = f"""KONU: {topic}

ARAÅžTIRMA VERÄ°LERÄ°:
{research[:6000]}

Bu verileri kullanarak profesyonel, veri destekli blog yazÄ±sÄ± yaz. Her iddiayÄ± araÅŸtÄ±rma verileriyle destekle."""

    return call_llm(client, system, user, temp=0.6)


def write_listicle(client: Groq, topic: str, research: str, images: Dict,
                   audience: str, tone: str, length: str,
                   statistics: List[str] = None, quotes: List[str] = None) -> str:
    lng = LENGTH_CONFIG[length]
    list_count = {"short": 5, "medium": 7, "long": 10}[length]
    image_md = _format_images(images)
    stats_md = _format_statistics(statistics or [])
    
    system = f"""Sen listicle uzmanÄ±sÄ±n. "{list_count} Yol/Strateji/Ä°pucu" formatÄ±nda yaz.

HEDEF: {lng['words']} kelime, {list_count} madde
TON: {TONE_CONFIG[tone]}

HER MADDE Ä°Ã‡Ä°N:
1. Dikkat Ã§ekici baÅŸlÄ±k (emoji ile)
2. 2-3 paragraf aÃ§Ä±klama
3. Somut Ã¶rnek veya istatistik (ZORUNLU)
4. Pro Tip kutusu

KULLANILACAK Ä°STATÄ°STÄ°KLER:
{stats_md}

GÃ–RSELLER: {image_md}"""

    user = f"""KONU: {topic}

ARAÅžTIRMA:
{research[:5000]}

Her maddede araÅŸtÄ±rmadan veri kullan."""

    return call_llm(client, system, user, temp=0.7)


def write_howto(client: Groq, topic: str, research: str, images: Dict,
                audience: str, tone: str, length: str,
                statistics: List[str] = None, quotes: List[str] = None) -> str:
    lng = LENGTH_CONFIG[length]
    step_count = {"short": 5, "medium": 7, "long": 10}[length]
    image_md = _format_images(images)
    
    system = f"""Sen teknik rehber yazarÄ±sÄ±n. AdÄ±m adÄ±m uygulama rehberi yaz.

HEDEF: {lng['words']} kelime, {step_count} adÄ±m

HER ADIM Ä°Ã‡Ä°N:
- â±ï¸ Tahmini sÃ¼re
- ðŸ“Š Zorluk seviyesi (Kolay/Orta/Zor)
- DetaylÄ± aÃ§Ä±klama
- ðŸ’¡ Ä°pucu kutusu
- âš ï¸ UyarÄ± kutusu (gerekirse)

EKSTRA BÃ–LÃœMLER:
- Gereksinimler listesi (baÅŸta)
- SÄ±k yapÄ±lan hatalar (sonda)
- Sorun giderme bÃ¶lÃ¼mÃ¼

GÃ–RSELLER: {image_md}"""

    user = f"""KONU: {topic}

ARAÅžTIRMA:
{research[:5000]}

Pratik, uygulanabilir rehber yaz."""

    return call_llm(client, system, user, temp=0.5)


def write_comparison(client: Groq, topic: str, research: str, images: Dict,
                     audience: str, tone: str, length: str,
                     statistics: List[str] = None, quotes: List[str] = None) -> str:
    lng = LENGTH_CONFIG[length]
    image_md = _format_images(images)
    stats_md = _format_statistics(statistics or [])
    
    system = f"""Sen karÅŸÄ±laÅŸtÄ±rma analisti yazarÄ±sÄ±n. DetaylÄ± X vs Y analizi yaz.

HEDEF: {lng['words']} kelime

YAPI:
1. HÄ±zlÄ± karÅŸÄ±laÅŸtÄ±rma tablosu (baÅŸta)
2. Her kriter iÃ§in detaylÄ± analiz
3. YÄ±ldÄ±z derecelendirmesi (â˜…â˜…â˜…â˜…â˜†)
4. ArtÄ±/Eksi listeleri
5. Her kriterde kazanan belirt
6. SonuÃ§: Kim neyi seÃ§meli

KULLANILACAK VERÄ°LER:
{stats_md}

GÃ–RSELLER: {image_md}"""

    user = f"""KONU: {topic}

ARAÅžTIRMA:
{research[:5000]}

Objektif, veri destekli karÅŸÄ±laÅŸtÄ±rma yaz."""

    return call_llm(client, system, user, temp=0.5)


def write_casestudy(client: Groq, topic: str, research: str, images: Dict,
                    audience: str, tone: str, length: str,
                    statistics: List[str] = None, quotes: List[str] = None) -> str:
    lng = LENGTH_CONFIG[length]
    image_md = _format_images(images)
    stats_md = _format_statistics(statistics or [])
    quotes_md = _format_quotes(quotes or [])
    
    system = f"""Sen vaka analisti yazarÄ±sÄ±n. DetaylÄ± vaka Ã§alÄ±ÅŸmasÄ± yaz.

HEDEF: {lng['words']} kelime

YAPI:
1. Ã–zet metrikleri tablosu (Ã–nce/Sonra)
2. Åžirket/KiÅŸi profili
3. Problem tanÄ±mÄ±
4. Ã‡Ã¶zÃ¼m aÅŸamalarÄ± (timeline)
5. SonuÃ§lar (rakamlarla)
6. Ã–ÄŸrenilen dersler
7. Uygulanabilir adÄ±mlar

KULLANILACAK VERÄ°LER:
{stats_md}

KULLANILACAK ALINTILAR:
{quotes_md}

GÃ–RSELLER: {image_md}"""

    user = f"""KONU: {topic}

ARAÅžTIRMA:
{research[:5000]}

GerÃ§ekÃ§i, veri destekli vaka Ã§alÄ±ÅŸmasÄ± yaz."""

    return call_llm(client, system, user, temp=0.6)


def run_final_editor(client: Groq, content: str, topic: str, format_type: str) -> str:
    format_info = FORMAT_CONFIG.get(format_type, FORMAT_CONFIG["standard"])
    
    system = f"""Sen baÅŸ editÃ¶rsÃ¼n. {format_info['name']} formatÄ±nÄ± son kez dÃ¼zenle.

GÃ–REVLER:
1. YazÄ±m ve dilbilgisi hatalarÄ±nÄ± dÃ¼zelt
2. CÃ¼mle akÄ±ÅŸÄ±nÄ± iyileÅŸtir
3. SEO iÃ§in baÅŸlÄ±klarÄ± optimize et
4. Meta description yaz
5. Anahtar kelimeleri belirle

Ã‡IKTI BAÅžINDA:
---
baslik: [SEO uyumlu baÅŸlÄ±k]
aciklama: [155 karakter meta description]
anahtar_kelimeler: [5-7 anahtar kelime]
okuma_suresi: [X dakika]
format: {format_type}
---

ArdÄ±ndan dÃ¼zenlenmiÅŸ iÃ§erik."""

    user = f"""KONU: {topic}

Ä°Ã‡ERÄ°K:
{content}

Final dÃ¼zenleme yap, SEO optimize et."""

    return call_llm(client, system, user, temp=0.2)


# ============================================================
# KALÄ°TE HESAPLAMA
# ============================================================

def calculate_readability_score(content: str) -> Dict:
    sentences = re.split(r'[.!?]+', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    words = content.split()
    
    if not sentences or not words:
        return {"score": 50, "level": "Orta", "level_color": "yellow"}
    
    avg_sentence_length = len(words) / len(sentences)
    long_words = sum(1 for w in words if len(w) > 12)
    long_word_ratio = long_words / len(words) * 100
    paragraphs = content.split('\n\n')
    avg_para_length = len(words) / max(len(paragraphs), 1)
    
    score = 100
    if avg_sentence_length > 25: score -= 20
    elif avg_sentence_length > 20: score -= 10
    if long_word_ratio > 15: score -= 15
    elif long_word_ratio > 10: score -= 8
    if avg_para_length > 100: score -= 10
    
    score = max(0, min(100, score))
    
    if score >= 80: level, color = "Ã‡ok Ä°yi", "green"
    elif score >= 60: level, color = "Ä°yi", "blue"
    elif score >= 40: level, color = "Orta", "yellow"
    else: level, color = "Zor", "red"
    
    return {"score": score, "level": level, "level_color": color}


def calculate_seo_score(content: str, topic: str) -> Dict:
    score = 50
    topic_lower = topic.lower()
    content_lower = content.lower()
    
    keyword_count = content_lower.count(topic_lower)
    if keyword_count >= 5: score += 20
    elif keyword_count >= 3: score += 15
    elif keyword_count >= 1: score += 8
    
    if content_lower[:500].find(topic_lower) != -1: score += 10
    
    headers = re.findall(r'^#{1,3}\s+.+', content, re.MULTILINE)
    if len(headers) >= 5: score += 10
    elif len(headers) >= 3: score += 5
    
    if re.search(r'!\[.+\]\(.+\)', content): score += 10
    if re.search(r'\[.+\]\(.+\)', content): score += 5
    
    # Veri kullanÄ±mÄ± bonusu
    if re.search(r'\d+%|\d+\s*(milyon|milyar)', content): score += 5
    
    score = max(0, min(100, score))
    
    if score >= 80: level, color = "MÃ¼kemmel", "green"
    elif score >= 60: level, color = "Ä°yi", "blue"
    elif score >= 40: level, color = "Orta", "yellow"
    else: level, color = "ZayÄ±f", "red"
    
    return {"score": score, "level": level, "level_color": color}


def calculate_fact_score(client: Groq, content: str, research: str) -> Dict:
    score = 60
    
    # AraÅŸtÄ±rma kullanÄ±mÄ±
    if research and len(research) > 1000: score += 15
    elif research and len(research) > 500: score += 10
    
    # Veri kullanÄ±mÄ±
    stats_count = len(re.findall(r'\d+%|\d+\s*(milyon|milyar|bin)', content))
    if stats_count >= 5: score += 15
    elif stats_count >= 3: score += 10
    elif stats_count >= 1: score += 5
    
    # Kaynak referanslarÄ±
    if re.search(r'(araÅŸtÄ±rma|rapor|Ã§alÄ±ÅŸma|anket)', content.lower()): score += 5
    
    # AlÄ±ntÄ± kullanÄ±mÄ±
    quote_count = len(re.findall(r'"[^"]{20,}"', content))
    if quote_count >= 2: score += 5
    
    score = min(100, score)
    
    if score >= 80: level, color = "GÃ¼venilir", "green"
    elif score >= 60: level, color = "Kabul Edilebilir", "blue"
    else: level, color = "Dikkatli Olun", "yellow"
    
    return {"score": score, "level": level, "level_color": color}


def calculate_originality_score(client: Groq, content: str, topic: str) -> Dict:
    score = 70
    
    if re.search(r'Ã¶rneÄŸin|mesela', content.lower()): score += 10
    if re.search(r'kendi deneyim|tecrÃ¼be', content.lower()): score += 5
    if len(content.split()) > 0 and len(set(content.split())) / len(content.split()) > 0.6: score += 10
    
    # Ã–zgÃ¼n bakÄ±ÅŸ aÃ§Ä±sÄ±
    if re.search(r'bence|kanÄ±mca|gÃ¶rÃ¼ÅŸÃ¼me gÃ¶re', content.lower()): score += 5
    
    score = min(100, score)
    
    if score >= 80: level, color = "Ã–zgÃ¼n", "green"
    elif score >= 60: level, color = "Ä°yi", "blue"
    else: level, color = "GeliÅŸtirilebilir", "yellow"
    
    return {"score": score, "level": level, "level_color": color}


def calculate_overall_quality(readability, seo, fact, originality) -> Dict:
    avg = (readability["score"] + seo["score"] + fact["score"] + originality["score"]) / 4
    
    if avg >= 80: level, color, grade = "MÃ¼kemmel", "green", "A"
    elif avg >= 70: level, color, grade = "Ã‡ok Ä°yi", "blue", "B+"
    elif avg >= 60: level, color, grade = "Ä°yi", "blue", "B"
    elif avg >= 50: level, color, grade = "Orta", "yellow", "C"
    else: level, color, grade = "GeliÅŸtirilebilir", "red", "D"
    
    return {"score": round(avg), "level": level, "level_color": color, "grade": grade}


# ============================================================
# STREAMING PIPELINE
# ============================================================

def run_blog_pipeline_streaming(
    topic: str,
    audience: str = "general",
    tone: str = "friendly",
    length: str = "medium",
    format_type: str = "standard"
) -> Generator[Dict[str, Any], None, None]:
    """
    Streaming blog pipeline - Her aÅŸamada event dÃ¶ndÃ¼rÃ¼r
    """
    
    client = Groq()
    format_info = FORMAT_CONFIG.get(format_type, FORMAT_CONFIG["standard"])
    
    results = {
        "topic": topic,
        "format": format_type,
        "research": "",
        "research_data": None,
        "images": {},
        "draft": "",
        "final": "",
        "quality": None,
    }
    
    total_steps = 5
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AGENT 1: DERÄ°N ARAÅžTIRMACI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "agent_start",
        "agent": AGENTS["researcher"],
        "step": 1,
        "total_steps": total_steps,
        "message": "7 katmanlÄ± derin araÅŸtÄ±rma baÅŸlatÄ±lÄ±yor..."
    }
    
    if SERPER_API_KEY:
        research_data = deep_research(topic, format_type)
        results["research"] = research_data["compiled_research"]
        results["research_data"] = research_data
        
        yield {
            "type": "agent_complete",
            "agent": AGENTS["researcher"],
            "step": 1,
            "total_steps": total_steps,
            "message": f"{research_data['sources_count']} kaynak, {len(research_data['statistics'])} istatistik bulundu",
            "data": {
                "sources_found": research_data["sources_count"],
                "statistics_count": len(research_data["statistics"]),
                "quotes_count": len(research_data["quotes"]),
                "layers": list(research_data["layers"].keys())
            }
        }
    else:
        yield {
            "type": "agent_complete",
            "agent": AGENTS["researcher"],
            "step": 1,
            "total_steps": total_steps,
            "message": "AraÅŸtÄ±rma atlandÄ± (API key yok)",
            "data": {"sources_found": 0}
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AGENT 2: GÃ–RSEL UZMANI
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "agent_start",
        "agent": AGENTS["visual_curator"],
        "step": 2,
        "total_steps": total_steps,
        "message": "GÃ¶rseller aranÄ±yor..."
    }
    
    if UNSPLASH_ACCESS_KEY:
        results["images"] = get_images_for_topic(topic, [topic])
        yield {
            "type": "agent_complete",
            "agent": AGENTS["visual_curator"],
            "step": 2,
            "total_steps": total_steps,
            "message": f"{len(results['images'])} gÃ¶rsel bulundu",
            "data": {"images_found": len(results["images"])}
        }
    else:
        yield {
            "type": "agent_complete",
            "agent": AGENTS["visual_curator"],
            "step": 2,
            "total_steps": total_steps,
            "message": "GÃ¶rsel arama atlandÄ±",
            "data": {"images_found": 0}
        }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AGENT 3: YAZAR
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "agent_start",
        "agent": AGENTS["writer"],
        "step": 3,
        "total_steps": total_steps,
        "message": f"{format_info['name']} formatÄ±nda veri destekli yazÄ±lÄ±yor..."
    }
    
    format_writers = {
        "standard": write_standard,
        "listicle": write_listicle,
        "howto": write_howto,
        "comparison": write_comparison,
        "casestudy": write_casestudy,
    }
    
    # Ä°statistik ve alÄ±ntÄ±larÄ± al
    statistics = []
    quotes = []
    if results["research_data"]:
        statistics = results["research_data"].get("statistics", [])
        quotes = results["research_data"].get("quotes", [])
    
    writer_func = format_writers.get(format_type, write_standard)
    results["draft"] = writer_func(
        client, topic, results["research"], results["images"],
        audience, tone, length, statistics, quotes
    )
    
    word_count = len(results["draft"].split())
    
    yield {
        "type": "agent_complete",
        "agent": AGENTS["writer"],
        "step": 3,
        "total_steps": total_steps,
        "message": f"Taslak hazÄ±r ({word_count} kelime)",
        "data": {"word_count": word_count}
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AGENT 4: EDITÃ–R
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "agent_start",
        "agent": AGENTS["editor"],
        "step": 4,
        "total_steps": total_steps,
        "message": "Final dÃ¼zenleme ve SEO optimizasyonu yapÄ±lÄ±yor..."
    }
    
    results["final"] = run_final_editor(client, results["draft"], topic, format_type)
    
    yield {
        "type": "agent_complete",
        "agent": AGENTS["editor"],
        "step": 4,
        "total_steps": total_steps,
        "message": "DÃ¼zenleme tamamlandÄ±",
        "data": {}
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # AGENT 5: KALÄ°TE ANALÄ°STÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "agent_start",
        "agent": AGENTS["quality_analyst"],
        "step": 5,
        "total_steps": total_steps,
        "message": "Kalite analizi yapÄ±lÄ±yor..."
    }
    
    readability = calculate_readability_score(results["final"])
    seo = calculate_seo_score(results["final"], topic)
    fact = calculate_fact_score(client, results["final"], results["research"])
    originality = calculate_originality_score(client, results["final"], topic)
    overall = calculate_overall_quality(readability, seo, fact, originality)
    
    results["quality"] = {
        "overall": overall,
        "readability": readability,
        "seo": seo,
        "fact_check": fact,
        "originality": originality
    }
    
    yield {
        "type": "agent_complete",
        "agent": AGENTS["quality_analyst"],
        "step": 5,
        "total_steps": total_steps,
        "message": f"Kalite skoru: {overall['score']}/100 ({overall['grade']})",
        "data": {"quality": results["quality"]}
    }
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FINAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    yield {
        "type": "final",
        "message": "Blog tamamlandÄ±!",
        "data": {
            "content": results["final"],
            "quality": results["quality"],
            "format": format_type,
            "word_count": len(results["final"].split()),
            "research_stats": {
                "sources": results["research_data"]["sources_count"] if results["research_data"] else 0,
                "statistics": len(results["research_data"]["statistics"]) if results["research_data"] else 0,
                "quotes": len(results["research_data"]["quotes"]) if results["research_data"] else 0
            }
        }
    }


# ============================================================
# NORMAL PIPELINE
# ============================================================

def run_blog_pipeline(
    topic: str,
    audience: str = "general",
    tone: str = "friendly",
    length: str = "medium",
    format_type: str = "standard",
    verbose: bool = True
) -> dict:
    """Normal (non-streaming) pipeline"""
    
    result = None
    for event in run_blog_pipeline_streaming(topic, audience, tone, length, format_type):
        if verbose:
            if event["type"] == "agent_start":
                print(f"\n{event['agent']['avatar']} {event['agent']['name']}: {event['message']}")
            elif event["type"] == "agent_complete":
                print(f"   âœ“ {event['message']}")
        
        if event["type"] == "final":
            result = {
                "topic": topic,
                "format": format_type,
                "final": event["data"]["content"],
                "quality": event["data"]["quality"]
            }
    
    return result


def get_agents_info() -> Dict:
    """Agent bilgilerini dÃ¶ndÃ¼r"""
    return AGENTS


def save_blog(content: str, topic: str) -> str:
    safe_topic = "".join(c if c.isalnum() or c in (' ', '-') else '' for c in topic)
    safe_topic = safe_topic.replace(' ', '-').lower()[:40]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"outputs/{safe_topic}_{timestamp}.md"
    
    os.makedirs("outputs", exist_ok=True)
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    
    return filename
